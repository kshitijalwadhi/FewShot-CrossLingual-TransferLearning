{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers.adapters.composition import Stack\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoAdapterModel\n",
    "from transformers import AdapterConfig\n",
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "def get_data(lang, data):\n",
    "    data = data[data[\"language\"] == lang]\n",
    "    return data\n",
    "\n",
    "def get_languages(data):\n",
    "    # data = pd.read_csv(\"data/train.tsv\",sep=\"\\t\")\n",
    "    return data[\"language\"].unique()\n",
    "\n",
    "def encode_batch(examples):\n",
    "    all_encoded = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for premise, hypothesis in (zip(examples[\"premise\"], examples[\"hypothesis\"])):\n",
    "        # encode separately to maintain CLS and SEP tokens\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            [premise, hypothesis],\n",
    "            add_special_tokens = True,\n",
    "            pad_to_max_length = True,\n",
    "            max_length = 100 ,\n",
    "            return_tensors = 'pt',\n",
    "            truncation = True\n",
    "            )\n",
    "\n",
    "        all_encoded[\"input_ids\"].append(encoded[\"input_ids\"].flatten())\n",
    "        all_encoded[\"attention_mask\"].append(encoded[\"attention_mask\"].flatten())\n",
    "\n",
    "    return all_encoded\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "  # Encode the input data\n",
    "  dataset = dataset.map(encode_batch, batched=True)\n",
    "  # The transformers model expects the target class column to be named \"labels\"\n",
    "  dataset = dataset.rename_column(\"gold_label\", \"labels\")\n",
    "  # Transform to pytorch tensors and only output the required columns\n",
    "  dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, lang):\n",
    "    en_data = get_data(lang, df)\n",
    "    en_data = en_data.dropna()\n",
    "    print(\"Size of data for language\", lang, en_data.shape)\n",
    "\n",
    "    labels = en_data[\"gold_label\"].values\n",
    "    labels = [0 if label == \"entailment\" else 1 if label == \"neutral\" else 2 for label in labels]\n",
    "    en_data[\"gold_label\"] = labels\n",
    "    en_data = Dataset.from_pandas(en_data)\n",
    "\n",
    "    dataset_en = preprocess_dataset(en_data)\n",
    "    dataset_en = dataset_en.remove_columns([\"language\", \"premise\", \"hypothesis\", \"__index_level_0__\"])\n",
    "\n",
    "    return dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dcd7ae17fa4396bed2bc5f5d09c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c82609518244c0abc9d84282f73c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m hi_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mजीवन एक चॉकलेट बॉक्स की तरह है।\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m chinese_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m生活就像一盒巧克力。\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m M2M100ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/m2m100_418M\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[39m=\u001b[39m M2M100Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/m2m100_418M\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/modeling_utils.py:2067\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   2065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2066\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 2067\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[1;32m   2069\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/modeling_utils.py:367\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    368\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    369\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:763\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    761\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    762\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39mmap_location)\n\u001b[0;32m--> 763\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    764\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:1100\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1098\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1099\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1100\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1102\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1104\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:1070\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1069\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1070\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1072\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:1048\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1046\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1048\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   1049\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1052\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1053\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate Hindi to French\n",
    "tokenizer.src_lang = \"hi\"\n",
    "encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100993</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Và, uh, nếu nó tăng và cứ tiếp tục tăng như vậ...</td>\n",
       "      <td>Nếu có sự thay đổi trong dòng điện, sẽ rất ngu...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100994</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>phải, tôi cũng không hiểu điều gì khiến, anh b...</td>\n",
       "      <td>Nó đạt tỉ lệ 99%.</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100995</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Anh ấy quay lại với Chúa Julian.</td>\n",
       "      <td>Anh muốn cầu xin Chúa Julian tha cho vợ mình.</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100996</th>\n",
       "      <td>entailment</td>\n",
       "      <td>Giờ đến lượt cô đang tự bào chữa, giọng cô run...</td>\n",
       "      <td>The woman was angry and defensive.</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100997</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Espinosa đã thu thập nhiều chuyện tình từ nhữn...</td>\n",
       "      <td>Espinosa bán lại những tác phẩm chuyện tình lã...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114988</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>Dans l'ordre juridique postbellum, le même rés...</td>\n",
       "      <td>L'ordre légal parabellum avait des résultats t...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114989</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Et euh, si ça montait brusquement et que ça co...</td>\n",
       "      <td>S'il y a une surtension d'électricité, c'est t...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114990</th>\n",
       "      <td>entailment</td>\n",
       "      <td>C'était toujours une zone culturelle mais la b...</td>\n",
       "      <td>La plus grande partie de la région était en ba...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114991</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>Ce que je pense de vous peut n'avoir que très ...</td>\n",
       "      <td>Tu devrais vraiment t'intéresser à ce que je p...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114992</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>Pour la communauté des nations, ainsi reconnue...</td>\n",
       "      <td>Il n'y a pas de droit de vote.</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gold_label                                            premise  \\\n",
       "100993        neutral  Và, uh, nếu nó tăng và cứ tiếp tục tăng như vậ...   \n",
       "100994  contradiction  phải, tôi cũng không hiểu điều gì khiến, anh b...   \n",
       "100995        neutral                   Anh ấy quay lại với Chúa Julian.   \n",
       "100996     entailment  Giờ đến lượt cô đang tự bào chữa, giọng cô run...   \n",
       "100997        neutral  Espinosa đã thu thập nhiều chuyện tình từ nhữn...   \n",
       "...               ...                                                ...   \n",
       "114988  contradiction  Dans l'ordre juridique postbellum, le même rés...   \n",
       "114989        neutral  Et euh, si ça montait brusquement et que ça co...   \n",
       "114990     entailment  C'était toujours une zone culturelle mais la b...   \n",
       "114991  contradiction  Ce que je pense de vous peut n'avoir que très ...   \n",
       "114992  contradiction  Pour la communauté des nations, ainsi reconnue...   \n",
       "\n",
       "                                               hypothesis language  \n",
       "100993  Nếu có sự thay đổi trong dòng điện, sẽ rất ngu...       vi  \n",
       "100994                                  Nó đạt tỉ lệ 99%.       vi  \n",
       "100995      Anh muốn cầu xin Chúa Julian tha cho vợ mình.       vi  \n",
       "100996                 The woman was angry and defensive.       vi  \n",
       "100997  Espinosa bán lại những tác phẩm chuyện tình lã...       vi  \n",
       "...                                                   ...      ...  \n",
       "114988  L'ordre légal parabellum avait des résultats t...       fr  \n",
       "114989  S'il y a une surtension d'électricité, c'est t...       fr  \n",
       "114990  La plus grande partie de la région était en ba...       fr  \n",
       "114991  Tu devrais vraiment t'intéresser à ce que je p...       fr  \n",
       "114992                     Il n'y a pas de droit de vote.       fr  \n",
       "\n",
       "[14000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"language\"] != \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
