{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navyajain/Desktop/IIT/Sem-7/COL772/A3/FewShot-CrossLingual-TransferLearning/a3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers.adapters.composition import Stack\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lang):\n",
    "    data = pd.read_csv(\"data/train.tsv\",sep=\"\\t\")\n",
    "    # take a subset 10%\n",
    "    data = data.sample(frac=0.1, random_state=42)\n",
    "    data = data[data[\"language\"] == lang]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages():\n",
    "    data = pd.read_csv(\"data/train.tsv\",sep=\"\\t\")\n",
    "    return data[\"language\"].unique()\n",
    "\n",
    "languages = get_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'vi', 'de', 'ar', 'es', 'bg', 'el', 'th', 'ru', 'tr', 'sw',\n",
       "       'ur', 'zh', 'hi', 'fr'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = get_data(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64432</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Not to agree to an unsettling and impossible t...</td>\n",
       "      <td>The purpose might be different to what is expe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85813</th>\n",
       "      <td>neutral</td>\n",
       "      <td>He wants to find the secret rational formula t...</td>\n",
       "      <td>AMC Movie Theaters knows the formula, that is ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92520</th>\n",
       "      <td>entailment</td>\n",
       "      <td>What is reputed to be Gen.</td>\n",
       "      <td>Gen's reputation is a mystery.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55356</th>\n",
       "      <td>neutral</td>\n",
       "      <td>it has worked out so he's got a real nice bene...</td>\n",
       "      <td>There are no downsides to being with a big com...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68875</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>Henceforth, this column will refer to the show...</td>\n",
       "      <td>The current name of the show is permanent.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gold_label                                            premise  \\\n",
       "64432        neutral  Not to agree to an unsettling and impossible t...   \n",
       "85813        neutral  He wants to find the secret rational formula t...   \n",
       "92520     entailment                         What is reputed to be Gen.   \n",
       "55356        neutral  it has worked out so he's got a real nice bene...   \n",
       "68875  contradiction  Henceforth, this column will refer to the show...   \n",
       "\n",
       "                                              hypothesis language  \n",
       "64432  The purpose might be different to what is expe...       en  \n",
       "85813  AMC Movie Theaters knows the formula, that is ...       en  \n",
       "92520                     Gen's reputation is a mystery.       en  \n",
       "55356  There are no downsides to being with a big com...       en  \n",
       "68875         The current name of the show is permanent.       en  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = en_data[\"gold_label\"].values\n",
    "labels = [0 if label == \"entailment\" else 1 if label == \"neutral\" else 2 for label in labels]\n",
    "en_data[\"gold_label\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = Dataset.from_pandas(en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gold_label', 'premise', 'hypothesis', 'language', '__index_level_0__'],\n",
       "    num_rows: 10162\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoAdapterModel\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdapterConfig\n",
    "\n",
    "# Load the language adapters\n",
    "lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n",
    "model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n",
    "model.load_adapter(\"de/wiki@ukp\", config=lang_adapter_config)\n",
    "\n",
    "# Add a new task adapter\n",
    "model.add_adapter(\"nli\")\n",
    "\n",
    "# Add a classification head for our target task\n",
    "model.add_multiple_choice_head(\"nli\", num_choices=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:05<00:00,  2.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "def encode_batch(examples):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  all_encoded = {\"input_ids\": [], \"attention_mask\": []}\n",
    "  # Iterate through all examples in this batch\n",
    "  for premise, hypothesis in (zip(examples[\"premise\"], examples[\"hypothesis\"])):\n",
    "    premise = [str(premise)+ \" \" + str(hypothesis) for _ in range(3)]\n",
    "    choices = [\"0\",\"1\",\"2\"]\n",
    "    encoded = tokenizer(\n",
    "        premise,\n",
    "        choices,\n",
    "        max_length=60,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    all_encoded[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "    all_encoded[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "  return all_encoded\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "  # Encode the input data\n",
    "  dataset = dataset.map(encode_batch, batched=True)\n",
    "  # The transformers model expects the target class column to be named \"labels\"\n",
    "  dataset = dataset.rename_column(\"gold_label\", \"labels\")\n",
    "  # Transform to pytorch tensors and only output the required columns\n",
    "  dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "  return dataset\n",
    "\n",
    "dataset_en = preprocess_dataset(en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = dataset_en.remove_columns([\"language\", \"premise\", \"hypothesis\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([\"nli\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active_adapters = Stack(\"en\", \"nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_en,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('a3': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6091885ef72d8bd65bb41724176428ea88e5de8e64830c7087eee2767bbf0f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
